# -*- coding: utf-8 -*-
"""LaporanML2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fawjLmIqWPIfD7Ayxl_A1otlfbTl_YS0

# Laporan Proyek Machine Learning - Damar Syarafi Ramadhan

Sistem Rekomendasi : Product Amazon

# Import

## Load Kagglehub
"""

!pip install kagglehub -q

import kagglehub

# Download latest version
path = kagglehub.dataset_download("karkavelrajaj/amazon-sales-dataset")

print("Path to dataset files:", path)

"""## Import library and file"""

import os
import pandas as pd

csv_path = os.path.join(path, "amazon.csv")
# Baca dataset
df = pd.read_csv(csv_path)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
from collections import Counter
from textblob import TextBlob

import string
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

#Feature Engineering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

#Model
from sklearn.metrics.pairwise import cosine_similarity

import warnings
warnings.filterwarnings('ignore')

df.head()

"""# Data Understanding"""

df.info()

"""Dataset Amazon Sales yang diambil dari Kaggle berisi informasi lengkap terkait produk dan ulasan pengguna, dengan struktur variabel sebagai berikut:
- product_id: ID unik produk
- product_name: Nama produk
- category: Kategori produk
- discounted_price: Harga produk setelah diskon
- actual_price: Harga asli produk sebelum diskon
- discount_percentage: Persentase diskon produk
- rating: Rating rata-rata produk
- rating_count: Jumlah pengguna yang memberikan rating
- about_product: Deskripsi produk
- user_id: ID pengguna yang memberikan review
- user_name: Nama pengguna yang memberikan review
- review_id: ID review pengguna
- review_title: Judul singkat review
- review_content: Isi review panjang
- img_link: Link gambar produk
- product_link: Link resmi produk di website Amazon

## Statistik dasar dataset
"""

df.describe().T

"""## Cek Nilai Null dan Duplicate"""

df.isna().sum()

df.duplicated().sum()

"""# Data Preparation

## Handling Missing Value & Duplicate
"""

clean_df = df.copy()

clean_df.isna().sum()

clean_df.loc[(clean_df['rating_count'].isna())]

clean_df['rating_count'].fillna(method='bfill', inplace=True) # Fill NaN with 0

clean_df.duplicated().sum()

"""Pengolahan Drop menggunakan imputasi dengan value sebelumnya.

## Feature Engineering
"""

# Convert 'discounted_price' and 'actual_price' by removing currency symbol and converting to float
clean_df['discounted_price'] = clean_df['discounted_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)
clean_df['actual_price'] = clean_df['actual_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)

# Convert 'discount_percentage' by removing '%' and converting to float
clean_df['discount_percentage'] = clean_df['discount_percentage'].astype(str).str.replace('%', '').astype(float)

# Convert 'rating' to float
clean_df['rating'] = pd.to_numeric(clean_df['rating'].astype(str).str.replace('|', ''), errors='coerce')

# Convert 'rating_count' by removing commas and converting to int
clean_df['rating_count'] = clean_df['rating_count'].astype(str).str.replace(',', '').astype(int)

clean_df.info()

"""## Text Cleaning"""

#Extracting the top-level category
print(f'Sebelum ekstraksi kategori utama {clean_df["category"].loc[0]}')
clean_df['category'] = clean_df['category'].apply(lambda x: x.split('|')[0] if pd.notnull(x) else x)
print(f'setelah ekstraksi kategori utama {clean_df["category"].loc[0]}')

# Cleaning and preprocessing text
def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation and special characters
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag
    text = re.sub(r'RT[\s]', '', text) # remove RT
    text = re.sub(r"http\S+", '', text) # remove link
    text = re.sub(r'[0-9]+', '', text) # remove numbers
    text = re.sub(r'[^\w\s]', '', text) # remove numbers

    text = text.replace('\n', ' ') # replace new line into space
    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations
    text = text.strip(' ') # remove characters space from both left and right text

    # Remove stopwords and apply lemmatization
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    filtered_words = []
    for word in text.split():
        if word not in stop_words:
            filtered_words.append(lemmatizer.lemmatize(word))

    return ' '.join(filtered_words)

# Assuming df is your DataFrame and it has been previously loaded
# Apply the clean_text function to the DataFrame columns
clean_df['product_name'] = clean_df['product_name'].apply(clean_text)
clean_df['about_product'] = clean_df['about_product'].apply(clean_text)
clean_df['review_title'] = clean_df['review_title'].apply(clean_text)
clean_df['review_content'] = clean_df['review_content'].apply(clean_text)
clean_df['category'] = clean_df['category'].apply(clean_text)
clean_df.head()

print(f'Contoh Sebelum Text Cleaning: {df["product_name"].loc[0]}')
print(f'Contoh Setelah Text Cleaning: {clean_df["product_name"].loc[0]}')

"""## IQR

**IQR** adalah konsep statistik yang terkait dengan distribusi data, dan penggunaannya untuk outlier adalah salah satu aplikasi utamanya. IQR mewakili rentang nilai yang mencakup 50% bagian tengah data Anda ketika diurutkan. Ini adalah ukuran penyebaran data yang "tahan" terhadap nilai-nilai ekstrem.

- Kuartil Pertama (Q1): Nilai di bawahnya terletak 25% data.
- Kuartil Ketiga (Q3): Nilai di bawahnya terletak 75% data (atau 25% data terletak di atasnya).
- IQR: Adalah perbedaan antara Kuartil Ketiga (Q3) dan Kuartil Pertama (Q1). IQR = Q3 - Q1
"""

numeric_cols = clean_df.select_dtypes(include=[np.number]).columns
numeric_cols

# Cek Outlier dengan IQR Outlier
def outlier_iqr(data):
    outliers = []
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    for i in data:
        if i < lower_bound or i > upper_bound:
            outliers.append(i)
    return outliers

print('Before Drop Outliers')
data_outlier = {}
for col in numeric_cols:
    data_outlier[col] = outlier_iqr(clean_df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

#Drop Outliers
def dropOutlier(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3-Q1

    lower_bound = Q1 - 1.5*IQR
    upper_bound = Q3 + 1.5*IQR

    df = np.where(df > upper_bound, upper_bound, df)
    df = np.where(df < lower_bound, lower_bound, df)
    return df

for col in numeric_cols:
    clean_df[col] = dropOutlier(clean_df[col])

print('After Drop Outliers')
data_outlier = {}
for col in numeric_cols:
    data_outlier[col] = outlier_iqr(clean_df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

"""# EDA

## Analisis Sentimen
"""

#Classify sentiment from review content
def sentiment_analysis(text):
    analysis = TextBlob(text)
    #threshold for positive and negative sentiments
    if analysis.sentiment.polarity > 0.1:
        return 'Positive'
    elif analysis.sentiment.polarity < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

#Applying sentiment analysis to the review content
reviews = clean_df['review_content']
reviews_sentiments = reviews.apply(sentiment_analysis)
clean_df['Sentiment'] = reviews_sentiments

label_encoder = LabelEncoder()

# Fitting the encoder and transforming the 'Sentiment' column
clean_df['Encoded_Sentiment'] = label_encoder.fit_transform(clean_df['Sentiment'])

# Finding examples of positive, neutral, and negative sentiments
positive_example = clean_df[clean_df['Sentiment'] == 'Positive'].iloc[0]['review_content']
neutral_example = clean_df[clean_df['Sentiment'] == 'Neutral'].iloc[0]['review_content']
negative_example = clean_df[clean_df['Sentiment'] == 'Negative'].iloc[0]['review_content']

print("Example of sentiment review: ")
example_reviews = pd.DataFrame({
    "Sentiment": ["Positive", "Neutral", "Negative"],
    "Review": [positive_example, neutral_example, negative_example]
})
example_reviews

# Counting the occurrences of each sentiment
sentiment_counts = reviews_sentiments.value_counts()

sentiment_counts.plot(kind='bar', color=['green', 'blue', 'red'], title='Sentiment Analysis of Review Content')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

"""**Insight yang didapatkan :**
- Terdapat pattern lonjakan yang sangat signifikan penjualan antara bulan November - Desember yang kemungkinan disebabkan promo akhir tahun.

## Top categories berdasarkan Jumlah Produk
"""

category_count = clean_df.groupby('category')['product_id'].count().reset_index()
category_count = category_count.sort_values(by='product_id', ascending=False)
category_count

# Rata-rata Rating Setiap Kategori
rating_cat = clean_df.groupby('category')['rating'].mean().reset_index()
rating_cat= rating_cat.sort_values(by='rating', ascending=False)
rating_cat

"""## Rata-rata diskon Setiap Kategori Product"""

# Rata-rata Diskon Setiap Kategori
category_disc = clean_df.groupby('category')['discounted_price'].mean().reset_index()
category_disc= category_disc.sort_values(by='discounted_price', ascending=False)
category_disc

# Rata-rata Diskon Setiap Kategori (Persentase)
per_category_disc = clean_df.groupby('category')['discount_percentage'].mean().reset_index()
per_category_disc= per_category_disc.sort_values(by='discount_percentage', ascending=False)
print(f'Rata-rata Diskon Setiap Kategori {per_category_disc}')
px.histogram(per_category_disc, x='category', y='discount_percentage', color='category')

"""## Heatmap Correlation"""

# Create a correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(clean_df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0, vmin=-1, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""**Insight yang didapatkan :**
- Pengerahan budget marketing sama rata pada semua kategori product.

# Model

Karena kita akan membuat model **Predictive Modeling** **Time Series** yang berhubungan dengan mengetahui data perkiraan terkait apa yang mungkin terjadi beberapa waktu mendatang.

## Term Frequency and Inverse Document Frequency

Menerapkan TF-IDF (Term Frequency dan Inverse Document Frequency) Sasaran utamanya adalah mengevaluasi pentingnya sebuah kata pada sebuah dokumen. Biasanya, terdiri dari:
- Term Frequency (TF): mengukur seberapa sering sebuah istilah muncul dalam sebuah dokumen. Dihitung dengan membagi jumlah kemunculan sebuah istilah dalam sebuah dokumen dengan jumlah total istilah dalam dokumen tersebut.
- Inverse Document Frequency (IDF): mengukur pentingnya istilah di seluruh korpus. Dihitung dengan mengambil logaritma dari jumlah total dokumen dibagi dengan jumlah dokumen yang memuat istilah tersebut.

Untuk menerapkan TF-IDF secara efektif pada kumpulan data ini, mari pertimbangkan penggabungan teks dari product_name, category, about_product untuk membentuk representasi komprehensif dari setiap produk.
"""

# Dropping irrelevant Features
drop_col = ['discounted_price', 'actual_price', 'discount_percentage', 'review_id', 'review_title',
                   'user_name', 'img_link', 'product_link']
drop_df = clean_df.drop(columns=drop_col)
drop_df.head()

#Instantiate TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
vectorizer.fit(drop_df['category'])

# Mapping array dari fitur index integer ke fitur nama
vectorizer.get_feature_names_out()

tfidf_matrix = vectorizer.fit_transform(drop_df['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape
# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# This block of code should be moved AFTER the cell where tfidf_matrix is calculated
# using the 'combined_text'.
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=vectorizer.get_feature_names_out(),
    index=drop_df.product_name).sample(5, axis=1).sample(10, axis=0)

"""## Content-based Approach"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=drop_df['product_name'], columns=drop_df['product_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def product_recommendations(nama_product, similarity_data=cosine_sim_df, items=drop_df[['product_name', 'category']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_product].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_product agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_product, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

contoh_product = drop_df['product_name'].sample().item()
print(contoh_product)

product_recommendations(contoh_product)

"""## Collaborative Recommendation"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = drop_df['user_id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah placeID menjadi list tanpa nilai yang sama
product_ids = drop_df['product_id'].unique().tolist()
print('list productID: ', product_ids)

# Melakukan proses encoding placeID
product_to_product_encoded = {x: i for i, x in enumerate(product_ids)}
print('encoded productID : ', product_to_product_encoded)

# Melakukan proses encoding angka ke placeID
product_encoded_to_product = {i: x for i, x in enumerate(product_ids)}
print('encoded angka ke productID: ', product_encoded_to_product)

# Mapping userID ke dataframe user
drop_df['user'] = drop_df['user_id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe resto
drop_df['product'] = drop_df['product_id'].map(product_to_product_encoded)

drop_df[['user', 'product']]

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_product = len(product_encoded_to_product)
print(num_product)

# Mengubah rating menjadi nilai float
# drop_df['rating'] = drop_df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(drop_df['rating'])

# Nilai maksimal rating
max_rating = max(drop_df['rating'])

print('Number of User: {}, Number of Products: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_product, min_rating, max_rating
))

# Mengacak dataset
sampled_df = drop_df.sample(frac=1, random_state=42)
sampled_df.head()

from sklearn.model_selection import train_test_split

X = sampled_df[['user', 'product']].values
y = sampled_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

print(pd.DataFrame(X_test).isna().sum())

# Convert X_test and y_test to pandas DataFrame/Series
X_test_df = pd.DataFrame(X_test)
y_test_series = pd.Series(y_test)

# Combine X_test and y_test to drop rows with NaN consistently
# This assumes the indices align between X_test and y_test
test_data = pd.concat([X_test_df, y_test_series.rename('target')], axis=1)

# Drop rows with any NaN values
cleaned_test_data = test_data.dropna()

# Separate X_test and y_test again
X_test_cleaned = cleaned_test_data.iloc[:, :-1].values
y_test_cleaned = cleaned_test_data['target'].values

print("Original X_test shape:", X_test.shape)
print("Cleaned X_test shape:", X_test_cleaned.shape)
print("Original y_test shape:", y_test.shape)
print("Cleaned y_test shape:", y_test_cleaned.shape)

import tensorflow as tf
import keras
from keras import layers
from keras import ops

class RecommenderNet(tf.keras.Model):
  # Insialisasi fungsi
  def __init__(self, num_users, num_product, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_product = num_product
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.product_embedding = layers.Embedding( # layer embeddings resto
        num_product,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.product_bias = layers.Embedding(num_product, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    product_vector = self.product_embedding(inputs[:, 1]) # memanggil layer embedding 3
    product_bias = self.product_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_product = tf.tensordot(user_vector, product_vector, 2)

    x = dot_user_product + user_bias + product_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_product, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)
model.summary()

history = model.fit(
    X_train, y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_test_cleaned, y_test_cleaned)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

df = sampled_df

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
product_bought_by_user = df[df.user_id == user_id]

product_not_bought = df[~df['product_id'].isin(product_bought_by_user.product_id.values)]['product_id']
product_not_bought = list(
    set(product_not_bought)
    .intersection(set(product_to_product_encoded.keys()))
)

product_not_bought = [[product_to_product_encoded.get(x)] for x in product_not_bought]
user_encoder = user_to_user_encoded.get(user_id)
user_product_array = np.hstack(
    ([[user_encoder]] * len(product_not_bought), product_not_bought)
)

ratings = model.predict(user_product_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_product_ids = [
    product_encoded_to_product.get(product_not_bought[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Product with high ratings from user')
print('----' * 8)

top_product_user = (
    product_bought_by_user.sort_values(
        by = 'rating',
        ascending=False
    ).head(5).product_id.values)

product_df_rows = df[df['product_id'].isin(top_product_user)]
for row in df.itertuples():
    print(row.product_name, ':', row.category)

print('----' * 8)
print('Top 10 Product recommendation')
print('----' * 8)

recommended_product = df[df['product_id'].isin(recommended_product_ids)]
for row in recommended_product.itertuples():
    print(row.product_name, ':', row.category)

"""## Hybrid Recommendation"""

# Compute the cosine similarity matrix based on the tfidf_matrix
cosine_sim = cosine_similarity(tfidf_matrix)

# Print the shape of the cosine similarity matrix to verify
cosine_sim.shape

# Create a product-user matrix with overall product ratings
product_user_matrix = df.pivot_table(index='product_id', values='rating', aggfunc='mean')

#Fill missing values with the average rating
product_user_matrix = product_user_matrix.fillna(product_user_matrix.mean())

#Display the product-user matrix
product_user_matrix.head()

def hybrid_recommendation(product_id, content_sim_matrix, product_user_matrix, products, top_n=10):
    #Get the index of the product that matches the product_id
    idx = products.index[products['product_id'] == product_id][0]

    #Content-based filtering
    #Get pairwise similarity scores
    sim_scores = list(enumerate(content_sim_matrix[idx]))
    #Sort the products based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    #Get the scores of the top N most similar products
    content_recommendations_idx = [i[0] for i in sim_scores[1:top_n+1]]

    #Collaborative Filtering
    #Get the rating of the current product
    if product_id in product_user_matrix.index:
        current_product_rating = product_user_matrix.loc[product_id].values[0]
        # Find products with similar ratings
        similar_rating_products = product_user_matrix.iloc[(product_user_matrix['rating']-current_product_rating).abs().argsort()[:top_n]]

    #Combine content and collaborative recommendations
    #Get indices for collaborative recommendations
    collaborative_recommendations_idx = similar_rating_products.index
    #Map indices to product IDs
    collaborative_recommendations_idx = [products.index[products['product_id'] == pid].tolist()[0] for pid in collaborative_recommendations_idx]

    #Combine indices from both methods and remove duplicates
    combined_indices = list(set(content_recommendations_idx + collaborative_recommendations_idx))

    #Get recommended products details
    recommended_products = products.iloc[combined_indices].copy()
    recommended_products = recommended_products[['product_id', 'product_name', 'category', 'rating']]

    return recommended_products

# Sample a single row from the DataFrame to get a sample product
sample_product_row = df[['product_id', 'product_name', 'category']].sample().iloc[0]

# Extract the product_id and product_name from the sampled row
sample_product_id_value = sample_product_row['product_id']
sample_product_name_value = sample_product_row['product_name']
sample_product_category_value = sample_product_row['category']

# Call the hybrid_recommendation function with the product_id value
recommended_products = hybrid_recommendation(sample_product_id_value, cosine_sim, product_user_matrix, df)

print("Recommendation for user who purchased product \"" + sample_product_name_value + "\"")
recommended_products.head(10)