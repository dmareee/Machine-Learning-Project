# -*- coding: utf-8 -*-
"""LaporanML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_wvfG4KTz5OzGU-leWLW3_-9J_6vyhC

# Proyek Predictive Analysis
# Laporan Proyek Machine Learning - Damar Syarafi Ramadhan
# Forecast Retail Sales Data

# Import

## Load Kagglehub
"""

!pip install kagglehub -q

import kagglehub

# Download latest version
path = kagglehub.dataset_download("abdullah0a/retail-sales-data-with-seasonal-trends-and-marketing/versions/1")

print("Path to dataset files:", path)

"""## Import library and file"""

import os
import pandas as pd

csv_path = os.path.join(path, "Retail_sales.csv")

# Baca dataset
df = pd.read_csv(csv_path)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

df.head()

"""# Data Understanding"""

df.info()

"""Isi Dataset berisikan :
- Sales Revenue (USD): Jumlah pendapatan yang didapat dari penjualan.
- Units Sold: Jumlah item produk terjual.
- Discount Percentage: Persentase Diskon yang dipasang pada produk tertentu.
- Marketing Spend (USD): Budget yang dikerahkan untuk promo marketing.
- Store ID: Identifier toko jual.
- Product Category: Kategori produk yang dijual (e.g., Electronics, Clothing).
- Date: Tanggal ketika penjualan terjadi.
- Store Location: Lokasi Geografis Toko.
- Day of the Week: Hari ketika penjualan terjadi.
- Holiday Effect: Indikator apakah penjualan terjadi selama periode liburan.
"""

df.rename(columns={'Store ID': 'store_ID',
                   'Product ID': 'product_ID',
                   'Store Name': 'store_name',
                   'Date':'date',
                   'Units Sold':'units_sold',
                   'Sales Revenue (USD)':'sales_revenue',
                   'Discount Percentage':'discount_percentage',
                   'Marketing Spend (USD)':'marketing_spend',
                   'Store Location':'store_location',
                   'Product Category':'product_category',
                   'Holiday Effect':'Holiday',
                   'Day of the Week':'Weekdays'}, inplace=True)
df.head()

df['date'] = pd.to_datetime(df['date'])
df['Month'] = df['date'].dt.month
df['Day'] = df['date'].dt.day
df['Year'] = df['date'].dt.year

df.drop(columns=['store_ID','product_ID',], inplace=True)

"""**Memisahkan format penanggalan menjadi fitur numerik agar dapat digunakan sebagai input model regresi nanti.**

Lalu Menghapus ID Store dan product yang tidak terpakai.
"""

df.describe(include='all').style.background_gradient('Greens_r')

df.isna().sum()

df.duplicated().sum()

"""## EDA

### Visualisasi tren penjualan harian untuk mengidentifikasi pola musiman dan outlier.
"""

# Keuntungan Sales Sehari-hari
sales_over_time = df.groupby('date')['sales_revenue'].sum()
print(sales_over_time)
# Visualisasi data
fig = px.line(sales_over_time, title='Sales Revenue Over Time', markers=True)

fig.update_layout(title='Sales Revenue Over Time',
                  xaxis_title='Holiday Effect',
                  yaxis_title='Sales Revenue (USD)')
fig.show()

"""**Insight yang didapatkan :**
- Terdapat pattern lonjakan yang sangat signifikan penjualan antara bulan November - Desember yang kemungkinan disebabkan promo akhir tahun.
"""

store_sales = df.groupby('store_location')['units_sold'].sum().reset_index()
store_sales.sort_values(by='units_sold', ascending=False)

"""### Rata-rata Marketing untuk Masing Product"""

# Penjualan per kategori
per_category_sales = df.groupby('product_category')['marketing_spend'].mean().reset_index()
per_category_sales.sort_values(by='marketing_spend', ascending=False)

"""**Insight yang didapatkan :**
- Pengerahan budget marketing sama rata pada semua kategori product.

### Penjualan Per Produk Per Bulan
"""

# Penjualan produk per bulan
product_sales_month = df.groupby(['Month', 'product_category'])[['sales_revenue', 'units_sold']].sum().reset_index()
#product_sales_month = product_sales_month.sort_values(by='sales_revenue', ascending=False)
product_sales_month

fig = px.line(product_sales_month, x='Month', y='sales_revenue', color='product_category')
fig.update_layout(title='Sales Revenue Over Month by Product Category',
                  xaxis_title='Month',
                  yaxis_title='Sales Revenue (USD)')
fig.show()

fig = px.line(product_sales_month, x='Month', y='units_sold', color='product_category')
fig.update_layout(title='Units Sold Over Month by Product Category',
                  xaxis_title='Month',
                  yaxis_title='Units Sould')
fig.show()

"""**Insight Yang didapatkan :**
- Saat lonjakan penjualan terjadi : Barang elektronik dan *furniture* mengalami penjualan yang drastis
- *Groceries* mengalami penjualan stagnan/stabil karena dibutuhkan setiap bulan.

### Penjualan Terjadi pada Hari libur
"""

holiday_sales = df.groupby('Holiday')['units_sold'].sum().reset_index()
holiday_sales

"""## Heatmap Correlation"""

# Create a correlation heatmap
numeric_cols_df = df.select_dtypes(include=[np.number])
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_cols_df.corr(), annot=True, cmap='coolwarm', center=0, vmin=-1, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""## Feature Correlation"""

fig = px.scatter_matrix(df,
                       dimensions=['marketing_spend', 'discount_percentage', 'sales_revenue'],
                       color='product_category',
                       title='Scatter Matrix of Marketing Spend, Discount Percentage, and Sales Revenue')
fig.show()

"""**Insight yang didapatkan :**
- Korelasi dengan Marketing Spend : Anggaran yang lebih tinggi kemungkinan besar menarik perhatian pelanggan ke produk atau toko, penjualan menjadi tinggi dan otomatis pendapatan menaik drastis.
- Korelasi dengan Diskon : Diskon biasanya bertujuan untuk mengurangi unit yang tersedia di gudang agar tidak *over capacity* sehingga mengurangi harga produk dengan persentase ditentukan, dengan resiko pendapatan berkurang dari harga biasa product.

# Data Preparation

## Handling Missing Value & Duplicate
"""

clean_df = df.copy()

clean_df.isna().sum()

clean_df.duplicated().sum()

"""Tidak ada Data Duplikat dan Kosong : **Tidak perlu Pengolahan *Drop* .**

## Label Encoder
"""

categorical_cols = ['store_location', 'product_category', 'Weekdays', 'Holiday']

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for col in categorical_cols:
    clean_df[col] = le.fit_transform(clean_df[col])

"""Menerapkan Labelisasi pada feature categorical sebagai representasi numerik sebagai input model nantinya.

## Standardization
"""

from sklearn.preprocessing import StandardScaler
numerical_cols = ['units_sold', 'discount_percentage', 'marketing_spend']
scaler = StandardScaler()
clean_df[numerical_cols] = scaler.fit_transform(clean_df[numerical_cols])
clean_df.head()

"""Proses untuk mengubah data agar memiliki format dan struktur yang konsisten sehingga dapat diproses dan dianalisis dengan lebih mudah dan efisien.

## IQR

**IQR** adalah konsep statistik yang terkait dengan distribusi data, dan penggunaannya untuk outlier adalah salah satu aplikasi utamanya. IQR mewakili rentang nilai yang mencakup 50% bagian tengah data Anda ketika diurutkan. Ini adalah ukuran penyebaran data yang "tahan" terhadap nilai-nilai ekstrem.

- Kuartil Pertama (Q1): Nilai di bawahnya terletak 25% data.
- Kuartil Ketiga (Q3): Nilai di bawahnya terletak 75% data (atau 25% data terletak di atasnya).
- IQR: Adalah perbedaan antara Kuartil Ketiga (Q3) dan Kuartil Pertama (Q1). IQR = Q3 - Q1
"""

cols = clean_df.columns

# Cek Outlier dengan IQR Outlier
def outlier_iqr(data):
    outliers = []
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    for i in data:
        if i < lower_bound or i > upper_bound:
            outliers.append(i)
    return outliers

print('Before Drop Outliers')
data_outlier = {}
for col in numeric_cols_df:
    data_outlier[col] = outlier_iqr(clean_df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

#Drop Outliers
def dropOutlier(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3-Q1

    lower_bound = Q1 - 1.5*IQR
    upper_bound = Q3 + 1.5*IQR

    df = np.where(df > upper_bound, upper_bound, df)
    df = np.where(df < lower_bound, lower_bound, df)
    return df

for col in numeric_cols_df:
    clean_df[col] = dropOutlier(clean_df[col])

print('After Drop Outliers')
data_outlier = {}
for col in numeric_cols_df:
    data_outlier[col] = outlier_iqr(clean_df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

"""## Train-Test Split

Langkah fundamental dalam alur kerja Machine Learning. Tujuan utamanya adalah untuk mengevaluasi kinerja model Anda pada data yang belum pernah dilihatnya selama pelatihan.
"""

cols

# A simple lag 1 feature
clean_df['sales_revenue_lag_1'] = clean_df['sales_revenue'].shift(1)
clean_df.head()

clean_df.dropna(inplace=True) # Add this line to drop rows with NaN

from sklearn.model_selection import train_test_split

features = ['units_sold', 'Day', 'Month', 'Year',
               'discount_percentage', 'marketing_spend', 'store_location',
               'product_category', 'Weekdays', 'Holiday',
               'sales_revenue_lag_1'] # Added the lagged feature
target = 'sales_revenue'

X = clean_df[features]  # Semua kolom kecuali target
y = clean_df[target]               # Kolom target

split_date = clean_df['date'].iloc[int(len(clean_df) * 0.8)]

X_train_ts = clean_df[clean_df['date'] < split_date][features]
y_train_ts = clean_df[clean_df['date'] < split_date][target]
X_test_ts = clean_df[clean_df['date'] >= split_date][features]
y_test_ts = clean_df[clean_df['date'] >= split_date][target]

# Ensure the shapes make sense
print(f"Train set shape: {X_train_ts.shape}, {y_train_ts.shape}")
print(f"Test set shape: {X_test_ts.shape}, {y_test_ts.shape}")

"""Untuk data deret waktu, Train-Test Split tidak boleh dilakukan secara acak. Data harus dibagi berdasarkan waktu secara berurutan. Bagian data yang lebih awal digunakan untuk pelatihan, dan bagian data yang lebih akhir digunakan untuk pengujian. Melakukan split acak akan menyebabkan data leakage"""

fig, ax = plt.subplots(figsize=(15, 5))
X_train_ts.plot(ax=ax, label='Training Set')
X_test_ts.plot(ax=ax, label='Test Set')
plt.show()

"""# Model : Regression

Karena kita akan membuat model **Predictive Modeling** **Time Series** yang berhubungan dengan mengetahui data perkiraan terkait apa yang mungkin terjadi beberapa waktu mendatang.
"""

from sklearn.metrics import mean_squared_error, r2_score
# Initialize list to collect results
results = []

# Function to evaluate models and store results for visualization
def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):

    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred_test = model.predict(X_test)

    # Calculate metrics: R² and RMSE
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    # Print results
    print(f"{model_name} Model:")
    print(f"Test R²: {test_r2:.4f}")
    print(f"Test RMSE: {test_rmse:.4f}\n")

    # Append results to the list for visualization
    results.append({'Model': model_name, 'R²': test_r2, 'RMSE': test_rmse})

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
# Initialize models
models = {
    'Linear Regression' : LinearRegression(),
    'Ridge Regression' : Ridge(),
    'Lasso Regression' : Lasso(),
    'ElasticNet Regression' : ElasticNet(),
    "XGBRegressor": XGBRegressor(),
}

"""## Evaluation"""

# Iterate over the models to evaluate each
for model_name, model in models.items():
    evaluate_model(model, X_train_ts, y_train_ts, X_test_ts, y_test_ts, model_name)

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

"""XGBRegressor menghasilkan skor **R Square** yang paling tinggi, maka diputuskan untuk menggunakan model XGBRegressor untuk **Predictive Model Time Series**."""

# Initialize and train a model (using XGBoost as it was the last one)
model_ts = XGBRegressor()

# Train the model using the time series split data
model_ts.fit(X_train_ts, y_train_ts)

# Make predictions on the test set (future data)
y_pred_test_ts = model_ts.predict(X_test_ts)

# Evaluate the model
test_r2_ts = r2_score(y_test_ts, y_pred_test_ts)
test_rmse_ts = np.sqrt(mean_squared_error(y_test_ts, y_pred_test_ts))

print("\nXGBoost Model with Time Series Features:")
print(f"Test R²: {test_r2_ts:.4f}")
print(f"Test RMSE: {test_rmse_ts:.4f}\n")

y_pred_test_ts

print("\n--- Checks before creating plot_df ---")
print("Shape of y_test_ts:", y_test_ts.shape)
print("Shape of y_pred_test_ts:", y_pred_test_ts.shape) # This is a numpy array

print("Are there NaNs in y_test_ts?", y_test_ts.isna().any())
print("Are there NaNs in y_pred_test_ts?", pd.Series(y_pred_test_ts).isna().any())

# Check the first few values of y_test_ts and y_pred_test_ts
print("y_test_ts head:\n", y_test_ts.head())
print("y_pred_test_ts head:\n", pd.Series(y_pred_test_ts).head()) # Convert to Series for head()

# Check the last few values
print("y_test_ts tail:\n", y_test_ts.tail())
print("y_pred_test_ts tail:\n", pd.Series(y_pred_test_ts).tail())

# Check if 'date' column is in clean_df
if 'date' not in clean_df.columns:
    print("Error: 'date' column not found in clean_df!")

# Check if any of the test indices are present in clean_df's index
print("Are test indices in clean_df index?", X_test_ts.index.isin(clean_df.index).all())

# After training the model and making predictions on the test set:
# with the correct date index for plotting
y_test_ts_array = y_test_ts.to_numpy()
#y_pred_test_ts = pd.Series(y_pred_test_ts, index=X_test_ts.index)

# Get the corresponding dates for the test set
# Assuming clean_df still has the 'date' column and the index aligns with X_test_ts
test_dates = clean_df.loc[X_test_ts.index, 'date']
test_dates

y_test_ts_array

y_pred_test_ts

# Create a DataFrame for easier plotting
plot_df = pd.DataFrame({
    'Actual Sales Revenue': y_test_ts_array,
    'Predicted Sales Revenue': y_pred_test_ts
}, index=test_dates) # Use the actual dates as the index

# Check the plot_df to see if it has data
print("plot_df head:\n", plot_df.head())
print("plot_df shape:", plot_df.shape)

# --- Plotting the results directly using plotly ---
import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Actual Sales Revenue'],
                          mode='lines+markers', name='Actual Sales Revenue'))

fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['Predicted Sales Revenue'],
                          mode='lines+markers', name='Predicted Sales Revenue',
                          line=dict(dash='dash')))

fig.update_layout(title='Actual vs. Predicted Sales Revenue (Test Set)',
                   xaxis_title='Date',
                   yaxis_title='Sales Revenue (USD)',
                   hovermode='x unified')

fig.show()